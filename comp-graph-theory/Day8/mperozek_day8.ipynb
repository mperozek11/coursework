{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b604009-a291-401d-a47f-afb6b6adb2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6850132-0e68-4570-a974-3eee2e501a29",
   "metadata": {},
   "source": [
    "0a) Download the same \"wikispeedia\" dataset from the PageRank homework assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeef87f0-94d4-424a-85c3-4f815a9478ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/maxperozek/CP341/Day4/wikispeedia_paths-and-graph/links.tsv'\n",
    "\n",
    "links_df = pd.read_csv(path, sep='\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcb939aa-8387-4631-92a0-e502252893cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "for index, row in links_df.iterrows():\n",
    "    edges.append((row['to'],row['from']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b71bc7ce-1efc-4c9d-be5b-41264c84e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_gr = nx.DiGraph()\n",
    "links_gr.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "984bb71e-cdbe-4450-8d5a-c50351d173b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc188ef7-569d-4406-9877-d4fb4c3c0d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = '/Users/maxperozek/CP341/Day4/plaintext_articles/'\n",
    "\n",
    "db = {}\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    \n",
    "    for file in files:\n",
    "        with open(rootdir + file, \"r\") as f:\n",
    "            text = f.read()\n",
    "            wordlist = text.split()\n",
    "            wordlist = set(wordlist)\n",
    "            wordlist = list(wordlist)\n",
    "            db[file[:-4]] = wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "962e7715-988a-46e6-8007-54e6d1cfc386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4604"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0015d28b-8103-405f-a614-6bec72cd18d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/maxperozek/CP341/Day4/wikispeedia_paths-and-graph/articles.tsv'\n",
    "idx_db = {}\n",
    "\n",
    "idx_df = pd.read_csv(path, sep='\\t', on_bad_lines='skip')\n",
    "\n",
    "for index, row in idx_df.iterrows():\n",
    "    idx_db[row['title']] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac613bfc-4e77-4eb1-bedc-15b1da4172e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_idx = idx_df.to_numpy().reshape((4604,\n",
    "                               ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e221183a-4999-4a5a-ac58-5689da2747bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4604,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73544d06-af9a-4849-bea3-e8ea863da69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%C3%81ed%C3%A1n_mac_Gabr%C3%A1in'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d90ac5a-7161-449f-8e30-7cdb9b744f80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1696"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_db['Ghana']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d5ae75-cf62-4f95-ba5f-52b3a43e6459",
   "metadata": {
    "tags": []
   },
   "source": [
    "Sample search:\n",
    "\n",
    "Ghana -> Atlantic_Ocean -> Baltic_Sea ->  Telephone\n",
    "\n",
    "Note: This may not be the shortest path, but this path does exist, so it should at least succeed when running search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d89891f-6012-4430-8212-cab9c63e606b",
   "metadata": {},
   "source": [
    "0b) Implement Dijkstra's algorithm, AKA bag-search with a priority queue. You should use the heapq package in python (which is already installed) as the priority queue. As we discussed in class, it's important to keep track of which nodes have been visited, along with their predecessor along the shortest path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22ed8a3c-3e3a-441b-ae0e-043e8f98503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pops lowest value first!\n",
    "# heapq.heappush(my_pqueue, (1.0, 'Dog'))\n",
    "\n",
    "# for backtracking from destination all the way back to src\n",
    "# backtracks FROM u TO v\n",
    "def get_path(gr, u, v):\n",
    "    path = []\n",
    "    cur = u\n",
    "    while cur:\n",
    "        path.append(cur)\n",
    "        cur = gr.nodes[cur]['prev']\n",
    "    return list(reversed(path))\n",
    "# don't forget to skip nodes we've already seen!!!\n",
    "def dijkstras(gr, src, dst):\n",
    "    gr = gr.copy()\n",
    "    pq = []\n",
    "    for node in gr:\n",
    "        if node == src:\n",
    "            gr.nodes[node]['dist'] = 0.0\n",
    "            gr.nodes[node]['prev'] = None\n",
    "            heapq.heappush(pq, (0.0, node))\n",
    "        else:\n",
    "            gr.nodes[node]['dist'] = np.inf\n",
    "            gr.nodes[node]['prev'] = None\n",
    "            heapq.heappush(pq, (np.inf, node))\n",
    "            \n",
    "    while pq:\n",
    "        v = heapq.heappop(pq)[1]\n",
    "        for neighbor in gr[v]:\n",
    "            new_dst = gr.nodes[v]['dist'] + 1 # note I am assuming that the distance of following a link is cost(1)\n",
    "            if new_dst < gr.nodes[neighbor]['dist']:\n",
    "                gr.nodes[neighbor]['dist'] = new_dst\n",
    "                gr.nodes[neighbor]['prev'] = v\n",
    "                heapq.heappush(pq, (new_dst, neighbor))\n",
    "        if v == dst:\n",
    "            return get_path(gr, dst, src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1f8e9ef-427d-4fce-85f3-9b55e56ec881",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = dijkstras(links_gr, 'Ghana', 'Telephone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07f291e7-b193-43cb-b1ea-6a6cd4ca82c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ghana', 'Atlantic_Ocean', 'Telephone']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec01834e-57c1-47ca-80f4-7647e2787d05",
   "metadata": {},
   "source": [
    "1a) As part of the PageRank homework, we kept track of a dictionary of which terms occurred on each page. Modify your implementation of Dijkstra's to use a terminology overlap heuristic. So, the priority of a page in our queue will be given by:\n",
    "If A and B are the two sets of words on the two pages, then a good starting heuristic to use is:\n",
    "len( A.intersection(B) ) / len( A.union(B) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f95710d-cb13-477b-8989-a9d810eb6e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a84d4ba-695a-427d-82d8-3883cea6c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-calculate heurisric matrix\n",
    "def h(u,v):\n",
    "    A = set(db[u])\n",
    "    B = set(db[v])\n",
    "    intersect_len = len(A.intersection(B))\n",
    "    union_len = len(A.union(B))\n",
    "    return intersect_len / union_len\n",
    "\n",
    "def calc_heuristic():\n",
    "    h_mat = -1 * np.ones((len(idx_df),len(idx_df)))\n",
    "\n",
    "    pbar = tqdm(total = h_mat.shape[0] * (h_mat.shape[0] -1), position=0, leave=True)\n",
    "    for i in range(h_mat.shape[0]):\n",
    "        for j in range(h_mat.shape[0]):\n",
    "            if(i != j and h_mat[j,i] == -1):\n",
    "                h_mat[i,j] = h(np_idx[i], np_idx[j])\n",
    "                pbar.update(1)\n",
    "    \n",
    "    h_mat_t = h_mat.T\n",
    "    for i in range(h_mat.shape[0]):\n",
    "        for j in range(h_mat.shape[0]):\n",
    "            if(i != j and h_mat[j,i] != -1):\n",
    "                h_mat[i,j] = h_mat_t[i,j]\n",
    "                pbar.update(1)\n",
    "    pbar.close()\n",
    "    return h_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16cd2f1a-843b-452f-9c1f-fb17bfb860c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 21192212/21192212 [49:39<00:00, 7112.68it/s]\n"
     ]
    }
   ],
   "source": [
    "h_mat = calc_heuristic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c747bc2f-6574-47ed-8a38-6a369512467e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxperozek/opt/anaconda3/envs/comp_gr_thy/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d739afb2-4023-4abd-82b2-40e4620611ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(h_mat, 'h_mat.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e04b472e-7432-428b-b215-4a8bb68602d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_mat = torch.load('h_mat.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80b1a3cf-975e-4687-8923-f2fbfae47490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def h(u,v):\n",
    "#     A = set(db[u])\n",
    "#     B = set(db[v])\n",
    "#     intersect_len = len(A.intersection(B))\n",
    "#     union_len = len(A.union(B))\n",
    "#     return intersect_len / union_len\n",
    "\n",
    "# for backtracking from destination all the way back to src\n",
    "# backtracks FROM u TO v\n",
    "def get_path(gr, u, v):\n",
    "    path = []\n",
    "    cur = u\n",
    "    while cur:\n",
    "        path.append(cur)\n",
    "        cur = gr.nodes[cur]['prev']\n",
    "    return list(reversed(path))\n",
    "# don't forget to skip nodes we've already seen!!!\n",
    "def astar(gr, src, dst):\n",
    "    gr = gr.copy()\n",
    "    pq = []\n",
    "    for node in gr:\n",
    "        if node == src:\n",
    "            gr.nodes[node]['dist'] = 0.0\n",
    "            gr.nodes[node]['prev'] = None\n",
    "            heapq.heappush(pq, (0.0, node))\n",
    "        else:\n",
    "            gr.nodes[node]['dist'] = np.inf\n",
    "            gr.nodes[node]['prev'] = None\n",
    "            heapq.heappush(pq, (np.inf, node))\n",
    "            \n",
    "    while pq:\n",
    "        v = heapq.heappop(pq)[1]\n",
    "        for neighbor in gr[v]:\n",
    "            # new_dst = gr.nodes[v]['dist'] + 1 + h(v, neighbor)\n",
    "            new_dst = gr.nodes[v]['dist'] + 1 + h_mat[idx_db[v], idx_db[neighbor]]\n",
    "\n",
    "            if new_dst < gr.nodes[neighbor]['dist']:\n",
    "                gr.nodes[neighbor]['dist'] = new_dst\n",
    "                gr.nodes[neighbor]['prev'] = v\n",
    "                heapq.heappush(pq, (new_dst, neighbor))\n",
    "\n",
    "        if v == dst:\n",
    "            return get_path(gr, dst, src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54052ec2-b898-4143-94ed-da8af115bd15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_path = astar(links_gr, 'Ghana', 'Telephone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01120f00-8cea-4985-be83-f70176ff5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = astar(links_gr, 'Ghana', 'Atlantic_Ocean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36f6b712-c6bd-4644-bc05-518cf4d94475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ghana', 'Atlantic_Ocean']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d22df-8a81-40fb-a74b-36eac408f30b",
   "metadata": {},
   "source": [
    "2a) the wikispeedia dataset contains a file of the shortest human-found paths between pairs of articles. Load these paths, and compare the lengths of a large number (~100) of paths to paths found by your methods in P0 and P1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4363ced9-8769-42e6-9d74-32c277583d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shortest_pathname = '/Users/maxperozek/CP341/Day4/wikispeedia_paths-and-graph/shortest-path-distance-matrix.txt'\n",
    "\n",
    "path_matrix = -1 * np.ones((4604, 4604)) # -1 indicates that there is no path from the surce to the target\n",
    "\n",
    "with open(shortest_pathname) as f:\n",
    "    row = 0\n",
    "    for line in f:\n",
    "        if line[0] != '#' and line[0] != '\\n':\n",
    "            line = ''.join(c for c in line if( c.isalnum() or c == '_'))\n",
    "            col = 0\n",
    "            for char in line:\n",
    "                # print(char)\n",
    "                if char != '_':  \n",
    "                    path_matrix[row,col] = char\n",
    "                col += 1\n",
    "            row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69ad1551-f43e-47e5-a990-08e777161cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1., -1., ...,  4.,  4.,  2.],\n",
       "       [-1.,  0., -1., ...,  3.,  3.,  3.],\n",
       "       [-1., -1.,  0., ...,  3.,  3.,  3.],\n",
       "       ...,\n",
       "       [-1., -1., -1., ...,  0.,  3.,  3.],\n",
       "       [-1., -1., -1., ...,  4.,  0.,  3.],\n",
       "       [-1., -1., -1., ...,  3.,  3.,  0.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6237d50d-2f52-4bcd-a0d9-01a72f8fa944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_matrix[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c97e0a9-d263-4fa0-ba8b-37d13b4d2ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "rand_nodes = np.random.choice(links_gr.nodes, 200, replace=False)\n",
    "srcs = rand_nodes[:100]\n",
    "dsts = rand_nodes[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6971a5a-c8be-4b2d-8742-e5e56984dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "total = srcs.shape[0]\n",
    "while i < total:\n",
    "    if path_matrix[ idx_db[srcs[i]], idx_db[dsts[i]] ] == -1:\n",
    "        srcs = np.delete(srcs, np.where(srcs == srcs[i]))\n",
    "        dsts = np.delete(dsts, np.where(dsts == dsts[i]))\n",
    "        total -= 1\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fad11243-a742-4ff2-9ee7-a79b35e828d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89,) (89,)\n"
     ]
    }
   ],
   "source": [
    "print(srcs.shape, dsts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f595f56-0fef-4fdd-b56c-7e7442a7daca",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = np.stack((srcs, dsts)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73e98169-db20-4d64-8ce0-d0ada221019d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f05207d7-225a-4200-8089-5e4bab410bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [00:53,  1.07it/s]                                  | 0/89 [00:00<?, ?it/s]\n",
      " 89%|██████████████████████████████████████▏    | 79/89 [01:14<00:07,  1.27it/s]\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "len_human = []\n",
    "len_dij = []\n",
    "len_heur = []\n",
    "\n",
    "pbar = tqdm(total=len(paths), position=0, leave=True)\n",
    "for p in paths:\n",
    "    len_human.append(path_matrix[ idx_db[p[0]], idx_db[p[1]] ])\n",
    "    len_dij.append(len(dijkstras(links_gr, p[0], p[1])))\n",
    "    len_heur.append(len(astar(links_gr, p[0], p[1])))\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b921b87-dc9c-4f46-833f-a805638162b6",
   "metadata": {},
   "source": [
    "Running got hung up on one of the last paths. I'm not really sure what's going on here... It never terminated on its own but the lists were still appended to through the point that I interrupted and are saved in memory, so I've observed the means below. My best guess is that memory is getting filled up with garbage throughout running this. I know that python does garbage collection for me but it's just a hunch, since I removed the paths that were unable to be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0bfccd9-045a-488a-a2f0-6f0b33d28c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b1511ae-8799-428a-8203-f0cbaa2cb21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.125"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(len_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b56452d0-3c82-4379-8ce3-029ef63965d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.15"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(len_dij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f2e0340-4597-43c2-9e92-47f8ae691b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.151898734177215"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(len_heur)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973a4915-4369-4985-bbde-571523705620",
   "metadata": {},
   "source": [
    "The human found best path seems to outperform my 'dijkstras' implementation by 1 link on average. I am waiting until I can leave my computer for a handful of hours before I calculate the heuristic lookup table and run the full heuristic search.\n",
    "One way that the heuristic could be improved would be making the calculation faster, perhaps processing the article text further to remove unimportant words and possibly even weight key words more heavily. Any way of decreasing the time of the expensive union and intersection operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca35f81-67aa-42b8-83b9-9fe9871af637",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
