{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9953c38e-9a69-4278-b687-309ba78dd0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54ef9f5-67ac-448e-8e32-836d8671d643",
   "metadata": {},
   "source": [
    "0a) Download the dataset from SNAP, and extract it somewhere:\n",
    "\n",
    "\n",
    "0b) Use links.tsv to construct a directed NetworkX graph. links.tsv looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9061e29c-5feb-47b1-96f4-1328b498b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15bc6d18-4e01-4cca-9c67-4042e1cdfa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/maxperozek/CP341/Day4/wikispeedia_paths-and-graph/links.tsv'\n",
    "\n",
    "links_df = pd.read_csv(path, sep='\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56427ef7-a96c-458e-bc3b-ea3f2ae2805d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>to</th>\n",
       "      <th>from</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>%C3%81ed%C3%A1n_mac_Gabr%C3%A1in</td>\n",
       "      <td>Bede</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>%C3%81ed%C3%A1n_mac_Gabr%C3%A1in</td>\n",
       "      <td>Columba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>%C3%81ed%C3%A1n_mac_Gabr%C3%A1in</td>\n",
       "      <td>D%C3%A1l_Riata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>%C3%81ed%C3%A1n_mac_Gabr%C3%A1in</td>\n",
       "      <td>Great_Britain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>%C3%81ed%C3%A1n_mac_Gabr%C3%A1in</td>\n",
       "      <td>Ireland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119877</th>\n",
       "      <td>Zulu</td>\n",
       "      <td>South_Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119878</th>\n",
       "      <td>Zulu</td>\n",
       "      <td>Swaziland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119879</th>\n",
       "      <td>Zulu</td>\n",
       "      <td>United_Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119880</th>\n",
       "      <td>Zulu</td>\n",
       "      <td>Zambia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119881</th>\n",
       "      <td>Zulu</td>\n",
       "      <td>Zimbabwe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119882 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      to            from\n",
       "0       %C3%81ed%C3%A1n_mac_Gabr%C3%A1in            Bede\n",
       "1       %C3%81ed%C3%A1n_mac_Gabr%C3%A1in         Columba\n",
       "2       %C3%81ed%C3%A1n_mac_Gabr%C3%A1in  D%C3%A1l_Riata\n",
       "3       %C3%81ed%C3%A1n_mac_Gabr%C3%A1in   Great_Britain\n",
       "4       %C3%81ed%C3%A1n_mac_Gabr%C3%A1in         Ireland\n",
       "...                                  ...             ...\n",
       "119877                              Zulu    South_Africa\n",
       "119878                              Zulu       Swaziland\n",
       "119879                              Zulu  United_Kingdom\n",
       "119880                              Zulu          Zambia\n",
       "119881                              Zulu        Zimbabwe\n",
       "\n",
       "[119882 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2ba95f9-90fd-4602-b4c4-d841aabfa50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "for index, row in links_df.iterrows():\n",
    "    edges.append((row['to'],row['from']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e16bf43b-fdf2-4cdf-98fa-e5be1f3d5153",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_gr = nx.DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cecb4fb0-0b95-4af2-80e1-9b96b6c18d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "links_gr.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "660f9d92-f5a6-4f4a-b91d-698475588170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links_gr.has_edge('%C3%89douard_Manet','Paris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1c61387-c379-4048-9bfc-1f27659a68b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.is_strongly_connected(links_gr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b1191-cd18-4dbc-8c3c-d8b83a78af89",
   "metadata": {},
   "source": [
    "0c) For every article, we want to build a database of search terms - what words show up in what article. You can do this any way you like. My recommendation would be to use a hash map that maps article title to a set of words. Pseudocode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4852632a-4415-44e5-945b-304aa86f93b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "154bfaaf-bf4c-4160-a3b5-69ecfb6947fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = '/Users/maxperozek/CP341/Day4/plaintext_articles/'\n",
    "\n",
    "db = {}\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    \n",
    "    for file in files:\n",
    "        with open(rootdir + file, \"r\") as f:\n",
    "            text = f.read()\n",
    "            wordlist = text.split()\n",
    "            wordlist = set(wordlist)\n",
    "            wordlist = list(wordlist)\n",
    "            db[file[:-4]] = wordlist\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8341cf7-50f1-4e10-a3a6-fbe7e9d6e166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4604"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a53a19-59ea-4335-a3f4-9530e8ab6227",
   "metadata": {},
   "source": [
    "0d) Write a line of code that uses your hashmap from 0c to list all of the article titles that contain a specific search term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be4896cf-cd24-41e9-a1a3-c351c3cd5fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Luminiferous_aether',\n",
       " 'Special_relativity',\n",
       " 'Introduction_to_special_relativity',\n",
       " 'Fermi_paradox',\n",
       " 'Black_hole',\n",
       " 'Metric_expansion_of_space',\n",
       " 'Gottfried_Leibniz',\n",
       " 'Hubble%27s_law',\n",
       " 'Physics',\n",
       " 'Euclidean_geometry',\n",
       " 'Quantum_mechanics',\n",
       " 'Redshift',\n",
       " 'Maxwell%27s_equations',\n",
       " 'Time',\n",
       " 'Acceleration',\n",
       " 'Big_Bang',\n",
       " 'Physical_paradox',\n",
       " 'String_theory',\n",
       " 'Gravitation',\n",
       " 'Phase_%28matter%29',\n",
       " 'Cosmic_inflation']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_word = 'spacetime'\n",
    "[key for key in db if search_word in db[key]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a81725b-3df3-4fb7-887c-10cf840858ee",
   "metadata": {},
   "source": [
    "1a) Write code to compute pageranks using a random traveler clicking links in the graph.  At each step, the traveler should either pick a random link on their current page, or (with a 15% chance) go to a random page somewhere else in the datasets. Perform a very large (more than 10^4) many hops and keep track of how many times your traveler visits each page in the graph. This number of visits, divided by the total number of visits, is the probability of our traveler visiting a given page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6263e876-3a84-4014-9228-a8e068c08884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "355a878f-956d-4771-9566-d0baa3a36cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_visits(gr, node):\n",
    "    if 'visits' in gr.nodes[node]:\n",
    "        gr.nodes[node]['visits'] = gr.nodes[node]['visits'] + 1\n",
    "    else:\n",
    "        gr.nodes[node]['visits'] = 1\n",
    "\n",
    "def rand_traveler_rank(gr, iters=int(1e4)):\n",
    "    gr = gr.copy()\n",
    "        \n",
    "    start = int(np.random.randint(low=0, high=len(links_gr.nodes()), size=1))\n",
    "    current = list(gr.nodes())[start]\n",
    "    inc_visits(gr, current)\n",
    "    \n",
    "    # walk iters\n",
    "    for i in range(iters):\n",
    "        \n",
    "        neighbors = list(gr[current])\n",
    "        if np.random.random() > 0.85 or len(neighbors) <= 0:\n",
    "            # go to a random page\n",
    "            current = list(gr.nodes())[int(np.random.randint(low=0, high=len(links_gr.nodes()), size=1))]\n",
    "            inc_visits(gr, current)\n",
    "        else:\n",
    "            new = neighbors[int(np.random.randint(low=0, high=len(neighbors), size=1))]\n",
    "            current = new\n",
    "            inc_visits(gr, current)\n",
    "    return gr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c12e49c-eb91-49b2-92da-d95f2a1ebb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_gr = rand_traveler_rank(links_gr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98382003-68c2-431b-af30-85607814744d",
   "metadata": {},
   "source": [
    "1b) Use your method from 0d, along with your ranks from 1a, to showcase a few example search results. Do these results look reasonable? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f725b4f1-d6c3-42d0-a1fc-702da60df8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranked_search(term, gr, db):\n",
    "    pages = [key for key in db if term in db[key]]\n",
    "    \n",
    "    ranked = []\n",
    "    for page in pages:\n",
    "        try:\n",
    "            ranked.append((gr.nodes().data()[page]['visits'], page)) if 'visits' in gr.nodes().data()[page] else ranked.append((0, page))\n",
    "        except KeyError:\n",
    "            ranked.append((0, page))\n",
    "    ranked.sort(key=lambda y: y[0], reverse=True)\n",
    "\n",
    "    return ranked\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5fc1c7-70e0-417c-887f-f98f6ae9a4f6",
   "metadata": {},
   "source": [
    "Looks like we are trying to use a node name that does not exist in the link graph\n",
    "the one that keeps popping up as an error is 'Private_Peaceful'\n",
    "I checked the original .tsv file and it is not in any of the edges\n",
    "#### Error Handling Options:\n",
    "When we see a page name that is not in the graph:\n",
    " * Add to graph as disconncected node with 'visits' = 0\n",
    " * Skip page, ignore completely\n",
    " * [Do not add to graph but still return it in the search results at the end of the list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fbf0e43-a456-4fbe-8155-96947bba5ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(99, 'United_States'),\n",
       " (51, 'English_language'),\n",
       " (45, 'Japan'),\n",
       " (41, 'World_War_II'),\n",
       " (34, 'World_War_I'),\n",
       " (30, 'Christianity'),\n",
       " (29, '19th_century'),\n",
       " (27, 'Scientific_classification'),\n",
       " (26, 'German_language'),\n",
       " (21, 'Judaism'),\n",
       " (21, 'Evolution'),\n",
       " (19, 'Bible'),\n",
       " (16, 'Berlin'),\n",
       " (16, 'Aristotle'),\n",
       " (14, 'Mathematics')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 15 results\n",
    "ranked_search('book', ranked_gr, db)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "959d8204-a31b-4eec-984d-4f0507d15668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(30, 'Time_zone'),\n",
       " (8, 'Logic'),\n",
       " (7, 'Quantum_mechanics'),\n",
       " (7, 'Asteroid'),\n",
       " (5, 'Prime_number'),\n",
       " (4, 'Time'),\n",
       " (4, 'Photon'),\n",
       " (3, 'Ordinary_differential_equation'),\n",
       " (3, 'Mathematical_analysis'),\n",
       " (2, 'Abacus'),\n",
       " (2, 'Cryptography'),\n",
       " (2, 'Greenhouse_effect'),\n",
       " (2, 'Differential_equation'),\n",
       " (2, 'Trigonometry'),\n",
       " (1, 'Calculus')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_search('compute', ranked_gr, db)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0fdf82c-94ae-4e48-9019-c9683e9fed7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(14, 'Mathematics'),\n",
       " (8, 'Isaac_Newton'),\n",
       " (8, 'Science'),\n",
       " (8, 'Education'),\n",
       " (8, 'Logic'),\n",
       " (6, 'Gottfried_Leibniz'),\n",
       " (4, 'Algorithm'),\n",
       " (4, 'Ren%C3%A9_Descartes'),\n",
       " (4, 'Programming_language'),\n",
       " (4, 'Geometry'),\n",
       " (4, 'Age_of_Enlightenment'),\n",
       " (3, 'Archimedes'),\n",
       " (3, 'Blaise_Pascal'),\n",
       " (3, 'Mathematical_analysis'),\n",
       " (2, 'Chemistry')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_search('calculus', ranked_gr, db)[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef185f4-075b-4232-97fd-196e599635b9",
   "metadata": {},
   "source": [
    "These search results look somewhat reasonable but in many cases, more 'common' things will be ranked higher than expected. The results are also very Euro-centric, but I imagine this is just the nature of this particular wikipedia subset. The most striking example of these two phenomena is the search for 'book', where top results are:\n",
    "'United_States', 'World_War_II', 'English_language', 'Japan', 'Christianity', 'World_War_I'\n",
    "With the exception of Japan, all of the top results are very western and it makes sense given the search that we have designed since the dataset is likely very western oriented, and there are probably tons of links going to 'United_States' and 'World_War_II'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5995155f-9229-47dd-9526-6d8d1325eab4",
   "metadata": {},
   "source": [
    "1c) Write code to compute pageranks using the matrix multiplication / Markov chain approach. We should still have a 15% chance of jumping to a random node in the network, rather than following links. I recommend implementing your matrix multiplication with sparse matrices - the transition matrix is probably too big to fit in memory otherwise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ff9eebf-94fb-413e-8f32-d24088f03c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse as spr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce351c23-9d50-4456-a977-20425b0ff7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we are treating source and target nodes correctly \n",
    "# adjacency matrix from networkx will probably need to be transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "964e2f0c-f3de-4556-ab7b-cef2757f48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = nx.to_scipy_sparse_array(links_gr).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "780cc1c8-ae06-47f6-b5a9-172ca73b8429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 12, 10, ...,  2,  4,  6])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "79d46957-2706-4ea4-ae22-aed36602486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_dense = adj.todense()\n",
    "sums = adj_dense.sum(0)\n",
    "\n",
    "# iter over cols\n",
    "for i in range(len(adj_dense)):\n",
    "    for j in range(len(adj_dense[0])):\n",
    "        adj_dense[i][j] = adj_dense[i][j] / sums[j] if not sums[j] == 0 else 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "10074c64-1dee-49f1-8cea-d9aae22f62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add handling for cols that sum to 0 (divide by 0 issue)\n",
    "\n",
    "# S = (adj / adj.sum(0))\n",
    "S_85 = spr.coo_matrix(0.85 * adj_dense)\n",
    "rand_coef = 0.15 * (1/len(adj_dense))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f891b1d3-4ad2-49d9-8d5d-f342da74e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = adj_dense\n",
    "goog = .85 * S + .15 * ((1/len(adj_dense)) * np.ones_like(S))\n",
    "goog = spr.coo_matrix(goog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "446a29b0-f40b-4ffe-818e-5823661c950d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00021777 0.00021777 0.00021777 ... 0.00021777 0.00021777 0.00021777]\n"
     ]
    }
   ],
   "source": [
    "current = 1/len(adj.todense()) * np.ones(len(adj.todense()))\n",
    "print(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1568cef0-84c3-45bb-acb5-7fd1c876a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "current = spr.csr_array(current).T\n",
    "current1 = current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "e2626f9a-d1dd-472a-9008-75d77880e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    # print(i, current.shape, current1.shape, goog.shape)\n",
    "    current1 = goog.dot(current1)\n",
    "    current = (S_85 * current) + (rand_coef * np.ones(current.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "553e3468-04f0-4565-92fc-a025fdcbf9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[3.26655052e-05],\n",
       "        [3.26655052e-05],\n",
       "        [3.26655052e-05],\n",
       "        ...,\n",
       "        [3.26655052e-05],\n",
       "        [3.26655052e-05],\n",
       "        [3.26655052e-05]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cbe1a7-1bc4-4345-a421-ff517845c582",
   "metadata": {},
   "source": [
    "1d) Same as 1b, but with your new ranks from 1c. Do these results look improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "2c159062-3454-4dd6-ad8a-b77635dc8b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranked_search_matrix(term, gr, db, vec):\n",
    "    pages = [key for key in db if term in db[key]]\n",
    "    \n",
    "    ranked = []\n",
    "    for page in pages:\n",
    "\n",
    "        try:\n",
    "            ranked.append((float(vec[list(gr.nodes()).index(page)].todense()), page))\n",
    "        except:\n",
    "            ranked.append((0, page))\n",
    "    \n",
    "    ranked = sorted(ranked, key= lambda x: x[0], reverse=True)\n",
    "    return ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "354b78f1-f038-4437-8665-5de61aeba8a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functional_programming\n",
      "Immanuel_Kant\n",
      "Animal_rights\n",
      "Japan\n",
      "John_W._Campbell\n",
      "Andriyivskyy_Descent\n",
      "Carl_Jung\n",
      "Arithmetic\n",
      "Book_of_Common_Prayer\n",
      "Wars_of_the_Roses\n",
      "Intelligence\n",
      "Martin_Luther_King%2C_Jr.\n",
      "Nintendo_Entertainment_System\n",
      "John_Dee\n",
      "Eifel_Aqueduct\n"
     ]
    }
   ],
   "source": [
    "search_res = ranked_search_matrix('book', links_gr, db, current1)\n",
    "for i in range(15):\n",
    "    print(search_res[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "cfeea226-6848-4501-bdc8-cfb3274896f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculus\n",
      "Charles_Babbage\n",
      "Abacus\n",
      "Pi\n",
      "Trigonometric_function\n",
      "Cryptography\n",
      "Quantum_mechanics\n",
      "StarCraft\n",
      "Time\n",
      "Time_zone\n",
      "Photon\n",
      "Actuary\n",
      "Greenhouse_effect\n",
      "String_theory\n",
      "Differential_equation\n"
     ]
    }
   ],
   "source": [
    "search_res = ranked_search_matrix('compute', links_gr, db, current1)\n",
    "for i in range(15):\n",
    "    print(search_res[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2a0f7883-7ba5-4bf5-8e81-923867a7511f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functional_programming\n",
      "Mathematics\n",
      "Chemistry\n",
      "Calculus\n",
      "Supply_and_demand\n",
      "Gottfried_Leibniz\n",
      "Isaac_Newton\n",
      "Algorithm\n",
      "Utilitarianism\n",
      "Pi\n",
      "Science\n",
      "Archimedes\n",
      "Force\n",
      "Ren%C3%A9_Descartes\n",
      "Programming_language\n"
     ]
    }
   ],
   "source": [
    "search_res = ranked_search_matrix('calculus', links_gr, db, current1)\n",
    "for i in range(15):\n",
    "    print(search_res[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd28927-0673-4a01-ba3c-ab93b8d70ada",
   "metadata": {},
   "source": [
    "The results from the markov chain strategy looks slightly improved from the random walk results. But there is still a great deal of overlap between the 2 strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd519b8-9255-4a24-87ac-22544ba0a8fb",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9badb77c-fe40-431e-b916-4122c34432b6",
   "metadata": {},
   "source": [
    "Given the ubiquity of search engines in the daily life of modern humans, and the consequent ability for search engines to influence and affect our perceptions of new information, there are heavy ethical implications to page rank algorithms which search engines use to prioritize some content over others. Cansu Canca’s “Did You Find It on the Internet? Ethical Complexities of Search Engine Rankings” gives the example of search results which perpetuate sexist stereotypes such as the conception that “professor” is a masculine profession, as search results show men in searches for “professor” at a disproportionately high rate. Canca cites other studies which have shown that young girls are interested in professions which have many female role models, something that is certainly lessened by search results which reflect dated/ stereotypical occupations based on gender. The Stanford Encyclopedia of Philosophy’s page “Search Engines and Ethics” provides a comprehensive overview of the topic but most interestingly notes that there is an ethical implication in the commercialization and opacity of page rank algorithms which will enable well funded corporations and groups to research and optimize their web pages for search, such that parties without such resources are pushed to the periphery of influence and accessibility via search engine results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc11b135-1cdf-41aa-8436-06f9b5e49ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
